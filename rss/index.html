<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Andrew Peng]]></title><description><![CDATA[Andrew Peng's Blog]]></description><link>https://andrewpeng.dev/</link><image><url>https://andrewpeng.dev/favicon.png</url><title>Andrew Peng</title><link>https://andrewpeng.dev/</link></image><generator>Ghost 2.28</generator><lastBuildDate>Tue, 26 Nov 2019 03:17:44 GMT</lastBuildDate><atom:link href="https://andrewpeng.dev/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Transformer [2/3]- PytorchTransformers Library]]></title><description><![CDATA[<p>In part 2 of my post, I'm going to go over huggingface's pytorch transformers library located <a href="https://github.com/huggingface/transformers">here</a>. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/transformers_logo_name-1.png" class="kg-image"></figure><!--kg-card-end: image--><p>This library provides over 30 pretrained state of the art transformer models on a variety of languages. Alike convolutional neural networks, transformers trained on a different linguistic dataset can be easily retrained on</p>]]></description><link>https://andrewpeng.dev/transformer-huggingface/</link><guid isPermaLink="false">5ddade70b2078d1be349f35f</guid><dc:creator><![CDATA[Andrew Peng]]></dc:creator><pubDate>Tue, 26 Nov 2019 03:12:41 GMT</pubDate><media:content url="https://andrewpeng.dev/content/images/2019/11/transformers_logo_name-3.png" medium="image"/><content:encoded><![CDATA[<img src="https://andrewpeng.dev/content/images/2019/11/transformers_logo_name-3.png" alt="Transformer [2/3]- PytorchTransformers Library"><p>In part 2 of my post, I'm going to go over huggingface's pytorch transformers library located <a href="https://github.com/huggingface/transformers">here</a>. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/transformers_logo_name-1.png" class="kg-image" alt="Transformer [2/3]- PytorchTransformers Library"></figure><!--kg-card-end: image--><p>This library provides over 30 pretrained state of the art transformer models on a variety of languages. Alike convolutional neural networks, transformers trained on a different linguistic dataset can be easily retrained on different language datasets. Using pretrained models rather than starting from scratch greatly reduces training time and can sometimes increase accuracy over training a model from scratch. </p><p>In this tutorial, I'll be fine-tuning a DistilBert model to predict the sentiment of IMDB movie reviews. DistilBert is a smaller version of the BERT model, allowing it to get most of the performance of BERT for much less training. More details are located in huggingface's <a href="https://medium.com/huggingface/distilbert-8cf3380435b5">blog post</a>. </p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="implementation">Implementation</h2><h3 id="data-preprocessing">Data Preprocessing</h3><p>I'm using an <a href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews">IMDB movie reviews dataset</a>, which has a list of movie reviews as well as either a "positive" or "negative" sentiment. </p><p>To use huggingface's pretrained models, we have to use their provided tokenizer. Because acquiring the sentiment from a review isn't reliant on stop words such as 'and', 'or', or 'at', we remove them. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">from bs4 import BeautifulSoup
from transformers import DistilBertTokenizer

from nltk.corpus import stopwords

stopwords = stopwords.words('english')
distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

review = BeautifulSoup(review, "html.parser").get_text()
review = review.lower()
review = distilbert_tokenizer.tokenize(review)

# Remove stopwords/punctuation
review = [w for w in review if w not in stopwords and contains_alphabet(w)]</code></pre><figcaption>Preprocess data class</figcaption></figure><!--kg-card-end: code--><p>Now in the dataset class, we attach the start token [CLF], insert padding, and convert the tokenized words to indicies. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">seq_length = 256
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

review = ['[CLF]'] + review[:seq_length - 1]
review = review + ['[PAD]' for _ in range(self.seq_length - len(review))]
review = tokenizer.convert_tokens_to_ids(review)</code></pre><figcaption>Dataset class</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h3 id="training">Training</h3><p>Instantiating the DistilBert model is as simple as importing the class. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2).to('cuda')
lrs = [{'params': model.distilbert.parameters(), 'lr': kwargs['lr_transformer']},
           {'params': model.pre_classifier.parameters()},
           {'params': model.classifier.parameters()}]
optim = Adam(lrs, lr=kwargs['lr_classifier'], eps=kwargs['eps'])</code></pre><figcaption>Train class. It may take a while to download the pretrained model. Here, I apply different learning rates to the transformer and classifier to achieve better results.&nbsp;</figcaption></figure><!--kg-card-end: code--><p>To train the model, all we have to do is pass the reviews and labels to the model and we get our losses back!</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python ">reviews = reviews.to('cuda')
labels = labels.to('cuda').long()

optim.zero_grad()
loss = model(reviews, labels=labels)[0]

loss.backward()
optim.step()</code></pre><figcaption>Train class. After I get the losses from every minibatch, I backpropagate.</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="results">Results</h2><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/Steps-and-Losses.png" class="kg-image" alt="Transformer [2/3]- PytorchTransformers Library"></figure><!--kg-card-end: image--><p>After 3 epochs, the pretrained transformer reaches a validation loss of 0.262 and a validation accuracy of 0.9021. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/Model-and-Validation-Accuracy.png" class="kg-image" alt="Transformer [2/3]- PytorchTransformers Library"></figure><!--kg-card-end: image--><p>I trained other common linguistic models including an LSTM and a transformer implemented using nn.Transformer. As shown in the graph, the huggingface transformer still edges out in terms of accuracy.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/Model-and-Training-time-per-epoch.png" class="kg-image" alt="Transformer [2/3]- PytorchTransformers Library"></figure><!--kg-card-end: image--><p></p><p>DistilBert took the longest time to train by far with approximately — min, likely because it has the most amount of parameters. This is followed by nn.Transformer and then by the LSTM. </p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="further-research">Further Research</h2><p>Some further improvements that could be made to this model:</p><ol><li>Improving the dataloader by allowing for variable length batches in order to reduce the amount of wasted memory spent on padding</li><li>Optimizing the parameters further, such as by adding differential learning rates</li></ol><p>My code is located <a href="https://github.com/andrewpeng02/imdb-sentiment">here</a>. </p>]]></content:encoded></item><item><title><![CDATA[Transformer [1/3]- Pytorch's nn.Transformer]]></title><description><![CDATA[<p>In part 1 of my series on transformers, I'm going to go over implementing a neural machine translation model using Pytorch's new nn.Transformer module. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/encoder-3.png" class="kg-image"></figure><!--kg-card-end: image--><p>Transformers, introduced by the paper <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need,</a> inherently don't have a sense of time, They instead rely on <strong>positional encoding </strong>to encode</p>]]></description><link>https://andrewpeng.dev/transformer-pytorch/</link><guid isPermaLink="false">5dc856062817c216af7800ec</guid><dc:creator><![CDATA[Andrew Peng]]></dc:creator><pubDate>Sun, 10 Nov 2019 20:11:25 GMT</pubDate><media:content url="https://andrewpeng.dev/content/images/2019/11/encoder-3-1.png" medium="image"/><content:encoded><![CDATA[<img src="https://andrewpeng.dev/content/images/2019/11/encoder-3-1.png" alt="Transformer [1/3]- Pytorch's nn.Transformer"><p>In part 1 of my series on transformers, I'm going to go over implementing a neural machine translation model using Pytorch's new nn.Transformer module. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/encoder-3.png" class="kg-image" alt="Transformer [1/3]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>Transformers, introduced by the paper <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need,</a> inherently don't have a sense of time, They instead rely on <strong>positional encoding </strong>to encode the order of elements. This gives the transformer architecture an important advantage over other language models such as recurrent neural networks: they are parallelizable and easy to expand. This has allowed huge models such as the 1.5 billion parameter <a href="https://openai.com/blog/better-language-models/">GPT-2</a> to achieve state of the art performance on language modelling. </p><h2 id="pytorch">Pytorch</h2><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/image.png" class="kg-image" alt="Transformer [1/3]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>Now, with the release of Pytorch 1.2, we can build transformers in pytorch! We'll go over the basics of the transformer architecture and how to use nn.Transformer. In a transformer, the input sentence goes through an encoder where the sentence gets passed through encoders to become memory. Then the output sentence and memory passes through decoders where it outputs the translated sentence. </p><h3 id="the-encoder">The Encoder</h3><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/encoder-1.png" class="kg-image" alt="Transformer [1/3]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>First, we tokenize the input data, pad the array if necessary, and convert the tokens to embeddings. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">import spacy

# Tokenize sentence
lang_model = spacy.load('en', disable=['tagger', 'parser', 'ner'])
sentence = sentence.lower()
sentence = [tok.text for tok in lang_model.tokenizer(sentence) if tok.text not in punctuation]

# Create a dictionary which maps tokens to indices (train contains all the training sentences)
freq_list = Counter()
    for sentence in train:
        freq_list.update(sentence)

# Convert tokens to indices
indices = [freq_list[word] for word in sentence if word in freq_list]</code></pre><figcaption>Here, I tokenize the sentence using spacy and convert the sentence to indices</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">import torch
from einops import rearrange

self.embed_src = nn.Embedding(vocab_size, d_model)
src = rearrange(indices, 'n s -&gt; s n')
src = self.embed_src(src)</code></pre><figcaption>In the LanguageTransformer class, I create an embedding and embed the batch</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/encoder-2.png" class="kg-image" alt="Transformer [1/3]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>Now we add the positional encoding to the sentences in order to give some order to the words. In the Attention is All You Need model, they use sine and cosine embeddings to give generalizability to longer sentence sizes. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">import math 
self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)
src = self.pos_enc(src * math.sqrt(self.d_model))</code></pre><figcaption>In the LanguageTransformer class, I scale src in order to reduce variance then apply the positional encoding</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python"># Source: https://pytorch.org/tutorials/beginner/transformer_tutorial
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=100):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_ter
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)</code></pre><figcaption>PositionalEncoding class</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/encoder-3-2.png" class="kg-image" alt="Transformer [1/3]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>Masking in the encoder is required to make sure any padding doesn't contribute to the self-attention mechanism. In Pytorch, this is done by passing src_key_padding_mask to the transformer. For the example, this looks like [False, False, False, False, False, False, False, True, True, True] where the True positions should be masked. The output of the encoder is called memory. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">for i, sentence in enumerate(batch):
        masks.append([False for _ in range(len(sentence))] + [True for _ in range(seq_length - len(sentence))])
        batch[i] = sentence + [0 for _ in range(seq_length - len(sentence))]</code></pre><figcaption>Padding and masking is taken care of in the dataset class</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h3 id="the-decoder">The Decoder</h3><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/decoder-1.png" class="kg-image" alt="Transformer [1/3]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>Now we can move onto the decoder architecture. The initial steps are very similar to that of the encoder. We embed and pass all but the very last token of each sentence into the decoders. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">self.embed_tgt = nn.Embedding(vocab_size, d_model)

tgt_inp = tgt[:, :-1]
tgt = rearrange(tgt_inp, 'n t -&gt; t n')
tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))</code></pre><figcaption>In the LanguageTransformer class, we embed and encode the target sequence</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/decoder-2.png" class="kg-image" alt="Transformer [1/3]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>We then pass these sequences through m decoders.  In each decoder, the sequences propagate through self attention and then attention with the memory (from the encoder). So the decoder requires 3 masks: </p><ol><li>tgt_mask: Used in the self-attention, it ensures the decoder doesn't look at future tokens from a given subsequence. This looks like [[0 -inf -inf ... ], [0 0 -inf ...] ... [0 0 0 ...]]</li><li>tgt_key_padding_mask: Also used in the self-attention, it ensures that the padding in the target sequence isn't accounted for. </li><li>memory_key_padding_mask: Used in the attention with the memory, it ensures the padding in the memory isn't used. This is the same as the src_key_padding_mask</li></ol><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">def gen_nopeek_mask(length):
    mask = rearrange(torch.triu(torch.ones(length, length)) == 1, 'h w -&gt; w h')
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))

    return mask

memory_key_padding_mask = src_key_padding_mask.clone()
tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to('cuda')</code></pre><figcaption>This is in the train method. src_key_padding_mask and tgt_key_padding_mask is expected from the dataloader</figcaption></figure><!--kg-card-end: code--><p>Afterwards, we pass each of the output sequences through a fully connected layer that outputs a probability for each token in the vocab size. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)

output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,
                                  tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)
output = rearrange(output, 't n e -&gt; n t e')
output = self.fc(output)</code></pre><figcaption>This is in the LanguageTransformer class</figcaption></figure><!--kg-card-end: code--><p>And here is the completed LanguageTransformer class!</p><!--kg-card-begin: code--><pre><code class="language-Python">class LanguageTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos_dropout, trans_dropout):
        super().__init__()
        self.d_model = d_model
        self.embed_src = nn.Embedding(vocab_size, d_model)
        self.embed_tgt = nn.Embedding(vocab_size, d_model)
        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)

        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)
        self.fc = nn.Linear(d_model, vocab_size)

    def forward(self, src, tgt, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_mask):
        src = rearrange(src, 'n s -&gt; s n')
        tgt = rearrange(tgt, 'n t -&gt; t n')
        src = self.pos_enc(self.embed_src(src) * math.sqrt(self.d_model))
        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))

        output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,
                                  tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)
        output = rearrange(output, 't n e -&gt; n t e')
        return self.fc(output)</code></pre><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h3 id="results">Results</h3><p>I used the tatoeba dataset, a small dataset with around 160000 english to french language pairs available <a href="http://www.manythings.org/anki/">here</a>. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/11/data.png" class="kg-image" alt="Transformer [1/3]- Pytorch's nn.Transformer"><figcaption>A relatively small dataset with short sentences</figcaption></figure><!--kg-card-end: image--><p>Here are the results of training for 20 epochs:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/Losses-and-Time.png" class="kg-image" alt="Transformer [1/3]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>My model achieves a validation loss of 0.99. However, it starts overfitting around epoch 15 based from the validation loss being higher than the train loss. And finally, some results of translating sentences:</p><!--kg-card-begin: markdown--><p>I am giving you a gift.: Je vous donne un cadeau.<br>
How did you find that?: Comment l'as-tu trouvée?<br>
I'm going to run to your house.: Je vais courir à votre maison.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="further-research">Further Research</h2><p>Some improvements that could be made:</p><ol><li>Using beam search to translate sentences</li><li>Running the model on larger datasets </li><li>Using torchtext instead of hacking my own dataset class to get more consistent batches</li><li>Using smoothened loss</li></ol><p>My code is located <a href="https://github.com/andrewpeng02/transformer-translation">here</a>. </p>]]></content:encoded></item><item><title><![CDATA[College Tuition Prediction [2/2]- Model]]></title><description><![CDATA[<p>Now that I've finished preparing the data, it's time to build the model!</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/ancient-architecture-bricks-220311-1.jpg" class="kg-image"></figure><!--kg-card-end: image--><p>I chose to try sklearn's support vector machine and random forest, as well as xgboost because predicting college tuition should be a relatively simple task. To find the best model, I used sklearn's GridSearchCV to brute force</p>]]></description><link>https://andrewpeng.dev/college-tuition-prediction-2/</link><guid isPermaLink="false">5d5a0776e0af2b6f96d80f7a</guid><dc:creator><![CDATA[Andrew Peng]]></dc:creator><pubDate>Mon, 19 Aug 2019 16:57:13 GMT</pubDate><media:content url="https://andrewpeng.dev/content/images/2019/08/ancient-architecture-bricks-220311.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://andrewpeng.dev/content/images/2019/08/ancient-architecture-bricks-220311.jpg" alt="College Tuition Prediction [2/2]- Model"><p>Now that I've finished preparing the data, it's time to build the model!</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/ancient-architecture-bricks-220311-1.jpg" class="kg-image" alt="College Tuition Prediction [2/2]- Model"></figure><!--kg-card-end: image--><p>I chose to try sklearn's support vector machine and random forest, as well as xgboost because predicting college tuition should be a relatively simple task. To find the best model, I used sklearn's GridSearchCV to brute force the parameters for each model. GridSearchCV takes the model (in this case a support vector regressor), a parameter grid (7*5*7=245 total combinations), a scoring method (regression so I'll use mean squared error), and number of folds for cross validation.</p><!--kg-card-begin: code--><pre><code class="language-python">model = svm.SVR()
grid_values = {'gamma': [0.01, 0.03, 0.1, 0.3, 1, 3, 9], 
               'C': [0.1, 0.3, 1, 3, 9], 
               'epsilon': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1]}
grid_search = GridSearchCV(model, param_grid=grid_values, n_jobs=-1, scoring='neg_mean_squared_error', cv=3)
grid_search.fit(X_train, y_train)</code></pre><!--kg-card-end: code--><p>Using GridSearchCV is similar for a random forest regressor and xgboost regressor, but it takes a different parameter grid. Here are the results:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/results.png" class="kg-image" alt="College Tuition Prediction [2/2]- Model"></figure><!--kg-card-end: image--><p>Unsurprisingly, the xgboost model did the best, followed by random forest and SVR. Here is a scatterplot of the xgboost model predicting on the test set:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/predictions.png" class="kg-image" alt="College Tuition Prediction [2/2]- Model"></figure><!--kg-card-end: image--><p>Visualizing the plot confirmed my hypothesis that the model would do best at lower tuition amounts. With xgboost, I can retrieve the feature importance and plot it as well.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/feature_importance.png" class="kg-image" alt="College Tuition Prediction [2/2]- Model"></figure><!--kg-card-end: image--><p>Admissions yield and professor salary contributed the most, while my categorical features regarding college size and location provided little predictive power.</p><p>Full code is located over on my <a href="https://github.com/andrewpeng02/college-tuition/tree/master">GitHub</a>.</p>]]></content:encoded></item><item><title><![CDATA[College Tuition Prediction [1/2]- Data Preparation]]></title><description><![CDATA[<p>Welcome to part 1 to a series of posts regarding my college tuition project!</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/delfi-de-la-rua-A_InfAQM_lU-unsplash-1.jpg" class="kg-image"></figure><!--kg-card-end: image--><p>In this project, I predict the tuition of colleges around the US using data from the <a href="https://nces.ed.gov/ipeds/use-the-data">National Center for Education Statistics</a>. I chose features I thought would best predict college tuition, such as admission rate, graduation</p>]]></description><link>https://andrewpeng.dev/college-tuition-prediction-1/</link><guid isPermaLink="false">5d5a0730e0af2b6f96d80f71</guid><dc:creator><![CDATA[Andrew Peng]]></dc:creator><pubDate>Mon, 19 Aug 2019 02:20:00 GMT</pubDate><media:content url="https://andrewpeng.dev/content/images/2019/08/delfi-de-la-rua-A_InfAQM_lU-unsplash-2.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://andrewpeng.dev/content/images/2019/08/delfi-de-la-rua-A_InfAQM_lU-unsplash-2.jpg" alt="College Tuition Prediction [1/2]- Data Preparation"><p>Welcome to part 1 to a series of posts regarding my college tuition project!</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/delfi-de-la-rua-A_InfAQM_lU-unsplash-1.jpg" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"></figure><!--kg-card-end: image--><p>In this project, I predict the tuition of colleges around the US using data from the <a href="https://nces.ed.gov/ipeds/use-the-data">National Center for Education Statistics</a>. I chose features I thought would best predict college tuition, such as admission rate, graduation rate, and college location, and I ended up with roughly 20 features.</p><h3 id="data-visualization">Data Visualization</h3><p>Always the first process you should do with your data is to visualize it. Here, I created various plots of tuition vs. various factors I thought would be the best predictors using Seaborn.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/08/tuition_hist.png" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"><figcaption>Histogram of Tuition</figcaption></figure><!--kg-card-end: image--><p>From this graph, I can tell that tuition is right skewed with peak of around 8000$.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/08/tuition_admissions.png" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"><figcaption>Scatterplot of Tuition and Admissions Rate</figcaption></figure><!--kg-card-end: image--><p>While admissions rate seems much more varied from 0-20000$, the rate appears to generally decrease.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/08/tuition_grad_rate.png" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"><figcaption>Scatterplot of Tuition and Graduation Rate</figcaption></figure><!--kg-card-end: image--><p>Again, a similar trend where graduation rate varies a lot from 0-20000$ but exhibits an increase afterwards.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/08/tuition_salary.png" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"><figcaption>Scatterplot of Tuition and Professor Salary</figcaption></figure><!--kg-card-end: image--><p>As expected, as tuition increases, professors' salaries also increases. From these graphs, I predict that my models will be able to predict college tuition more accurately around 30,000$ because of its lower variability in factors like graduation rate.</p><h3 id="data-preparation">Data Preparation</h3><p>The data.csv file contains the colleges as rows and features (including tuition) as columns. I will be using Pandas for loading data and scikit-learn for preprocessing.</p><!--kg-card-begin: code--><pre><code class="language-python">import pandas as pd
import joblib

import sklearn.preprocessing as preprocessing
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer</code></pre><!--kg-card-end: code--><p>I removed the colleges that didn't have tuition filled in, as well as the rows which have less than 18 out of the 25 features so that there are enough features to predict tuition. This left about 2900 colleges from the original total of 7000.</p><!--kg-card-begin: code--><pre><code class="language-python">data = pd.read_csv('data/data.csv')
data = data[pd.notnull(data['Tuition and fees'])]

min_fields = 18
for index, row in data.iterrows():
    filled = 0

    for name, field in row.items():
        if str(field) != 'nan':
            filled += 1

    if filled &lt;= min_fields:
        data.drop(index, inplace=True)
        data.drop(data.columns[[0, 1, 2]], axis=1, inplace=True)</code></pre><!--kg-card-end: code--><p>Split data into train and test sets using train_test_split() with 80% of the data going into the train set.</p><!--kg-card-begin: code--><pre><code>data = data.reset_index(drop=True)
train, test = train_test_split(data, test_size=0.2)
X_train, y_train = train.drop(train.columns[[2]], axis=1), train.iloc[:,2]
X_test, y_test = test.drop(test.columns[[2]], axis=1), test.iloc[:, 2]</code></pre><!--kg-card-end: code--><p>Scale targets between 0 and 1 for faster convergence. While the sklearn libary contains a plethora of scalers, I chose min max scaler because of its simplicity and skewed dataset. I saved the scaler using joblib so I could later scale the predicted targets back.</p><!--kg-card-begin: code--><pre><code class="language-python">scaler = preprocessing.MinMaxScaler()
scaler.fit(y_train.values.reshape(-1, 1))
y_train, y_test = scaler.transform(y_train.values.reshape(-1, 1)), scaler.transform(y_test.values.reshape(-1, 1))
joblib.dump(scaler, 'min_max_scaler.pkl')
</code></pre><!--kg-card-end: code--><p>To encode the numeric features, I defined the columns which had numeric features and a Pipeline from sklearn. I first imputed the data (meaning I replaced missing values with the mean) and then scaled the data. For the categorical features, I instead used a OneHotEncoder in a Pipeline to convert the categories to numeric values.</p><!--kg-card-begin: code--><pre><code class="language-python">numeric_features = X_train.columns[[0, 1] + [i for i in range(4, 24)]]
numeric_trans = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),
('scaler', preprocessing.MinMaxScaler())])

column_features = X_train.columns[[2, 3]]
column_trans = Pipeline(steps=[('encoder', preprocessing.OneHotEncoder(drop='first'))])</code></pre><!--kg-card-end: code--><p>I then created a ColumnTransformer by passing the pipelines and feature names and then transformed the data.</p><!--kg-card-begin: code--><pre><code>transformer = ColumnTransformer(transformers=[('numeric', numeric_trans, numeric_features),
('categorical', column_trans, column_features)],
remainder='passthrough')
transformer.fit(X_train)

X_train = pd.DataFrame(transformer.transform(X_train))
X_test = pd.DataFrame(transformer.transform(X_test))</code></pre><!--kg-card-end: code--><p>Now that the data is ready, it's finally time for creating the model in <a href="https://andrewpeng.dev/college-tuition-prediction-1/andrewpeng.dev/college-tuition-prediction-2">Part 2</a>!</p><p><br></p><p>Full code is located over on my <a href="https://github.com/andrewpeng02/college-tuition/tree/master">GitHub</a>.</p>]]></content:encoded></item></channel></rss>