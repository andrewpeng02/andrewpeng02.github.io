<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Andrew Peng]]></title><description><![CDATA[Andrew Peng's Blog]]></description><link>https://andrewpeng.dev/</link><image><url>https://andrewpeng.dev/favicon.png</url><title>Andrew Peng</title><link>https://andrewpeng.dev/</link></image><generator>Ghost 2.28</generator><lastBuildDate>Sun, 01 Mar 2020 19:56:00 GMT</lastBuildDate><atom:link href="https://andrewpeng.dev/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Deep Reinforcement Learning [1/4]- Deep Q Learning]]></title><description><![CDATA[<p>Welcome to my first post in a series on deep reinforcement learning in Pytorch. Reinforcement learning is a branch of machine learning dealing with agents and how they make decisions in an environment. RL can be used to play games from snake to <a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">Starcraft</a> or even to trade stocks. </p><!--kg-card-begin: html--><iframe src="https://giphy.com/embed/ZaWuYUwkdzIMozwsvR" width="480" height="320" frameborder="0" class="giphy-embed" allowfullscreen></iframe><center><font size="3">OpenAI</font></center>]]></description><link>https://andrewpeng.dev/deeprl-dql/</link><guid isPermaLink="false">5e49a6b661f14b1e187d36c8</guid><dc:creator><![CDATA[Andrew Peng]]></dc:creator><pubDate>Sun, 01 Mar 2020 19:54:24 GMT</pubDate><media:content url="https://andrewpeng.dev/content/images/2020/03/Screenshot-from-2020-03-01-13-53-50.png" medium="image"/><content:encoded><![CDATA[<img src="https://andrewpeng.dev/content/images/2020/03/Screenshot-from-2020-03-01-13-53-50.png" alt="Deep Reinforcement Learning [1/4]- Deep Q Learning"><p>Welcome to my first post in a series on deep reinforcement learning in Pytorch. Reinforcement learning is a branch of machine learning dealing with agents and how they make decisions in an environment. RL can be used to play games from snake to <a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">Starcraft</a> or even to trade stocks. </p><!--kg-card-begin: html--><iframe src="https://giphy.com/embed/ZaWuYUwkdzIMozwsvR" width="480" height="320" frameborder="0" class="giphy-embed" allowfullscreen></iframe><center><font size="3">OpenAI Gym's Lunar Lander Environment</font></center><!--kg-card-end: html--><p>In this post, I'm going to teach you how to implement deep q learning, a simple deep reinforcement learning algorithm, in Pytorch. </p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="terminology">Terminology</h2><p>The environment is where the agent makes decisions, which is represented by a state vector. The goal of the agent is to maximize total reward. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2020/02/env_step.png" class="kg-image" alt="Deep Reinforcement Learning [1/4]- Deep Q Learning"></figure><!--kg-card-end: image--><p>At timestep \(t=0\), the environment has a state \(s_0\). The agent then takes an action \(a_0\) according to a policy \(\pi\) (which just tells the agent to take an action from a given state), and receives a reward \(r_0\) and the next state \(s_1\). One way of finding a good policy is using deep Q learning. Each episode consists of all state, actions, and rewards from beginning to end.</p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="the-algorithm">The Algorithm</h2><p>Reinforcement learning using neural networks is unstable and difficult to train. However, <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">deep Q learning</a> introduces several improvements which makes deep reinforcement learning more feasible:</p><ol><li>It uses a replay buffer, which stores the last \(n\) experiences that consist of a state, action, reward, next state, and done tuple. The learning is based by sampling minibatches from the replay buffer, thereby removing the high correlation between experiences.</li><li>It uses off-policy learning. There are 2 Q networks, a local (which is being optimized) and a target (periodically updated by the local network) network. Learning off-policy helps decrease correlation between the local and target expected rewards (explained later). </li></ol><h3 id="q-networks">Q Networks</h3><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2020/02/nn-1.png" class="kg-image" alt="Deep Reinforcement Learning [1/4]- Deep Q Learning"><figcaption>Example Q network for the Lunar Lander environment</figcaption></figure><!--kg-card-end: image--><p>In deep Q learning, Q networks are neural networks trained to select the best action. The input to a Q network is the state vector \(s\) and the output is the action vector \(a\). We train the Q network to output the expected return of each action given a state, so that the action with the highest expected return should be selected. The expected return is just the sum \(\sum_{i=0}^{j} P(R_i) * R_i\) where \(R_i=r_t+r_{t+1}+r_{t+2}...\). </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">class DQNModel(nn.Module):
    def __init__(self, state_len, action_len):
        super().__init__()

        self.fc1 = nn.Linear(state_len, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_len)

        self.relu = nn.ReLU()

    def forward(self, x):
        # Takes in a state vector with length state_len and outputs an action vector with length action_len
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)

        return x</code></pre><figcaption>This is an example of a simple Q network which inputs the state vector and outputs the expected returns for each of the possible actions.</figcaption></figure><!--kg-card-end: code--><h3 id="the-policy">The Policy</h3><p>Deep Q learning uses the epsilon greedy policy. A random action is chosen with probability \(\epsilon\) and an action chosen according to the Q network is chosen with probability \(1-\epsilon\). Epsilon is usually decayed towards 0 so that gradually more of the actions are taken according to the Q network.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">    def policy(self, state, eps):
        if random.random() &lt; eps:
            # Random action with probability epsilon
            return self.env.action_space.sample()
        else:
            # Act according to local q network by selecting the action with highest expected return
            self.local_qnetwork.eval()
            with torch.no_grad():
            	state = torch.FloatTensor(state).cuda().unsqueeze(0)
                out = self.local_qnetwork(state).cpu()
            self.local_qnetwork.train()

            return torch.argmax(out).item()</code></pre><figcaption>This is the policy method, which accepts a state and the epsilon and outputs the action.</figcaption></figure><!--kg-card-end: code--><h3 id="training-the-q-network">Training the Q network</h3><p>The paper introduces replay buffers which simply hold the most recent experience tuples and act as our dataloader. We then train the network on minibatches sampled from the replay buffer. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">class ReplayBuffer:
    def __init__(self, queue_len, device):
        self.queue = deque(maxlen=queue_len)
        self.device = device

    def put(self, experience):
        self.queue.append(experience)

    def batch_get(self, batch_size, state_size):
        if batch_size &gt; len(self.queue):
            experiences = [random.sample(self.queue, 1)[0] for _ in range(batch_size)]
        else:
            experiences = random.sample(self.queue, batch_size)

        states, next_states = torch.zeros((batch_size, state_size)), torch.zeros((batch_size, state_size))
        actions, rewards, dones = torch.zeros((batch_size, 1)), torch.zeros((batch_size, 1)), torch.zeros((batch_size, 1))
        for i, experience in enumerate(experiences):
            states[i] = torch.FloatTensor(experience[0])
            actions[i] = experience[1]
            rewards[i] = experience[2]
            next_states[i] = torch.FloatTensor(experience[3])
            dones[i] = experience[4]
        return states.to(self.device), actions.to(self.device), rewards.to(self.device),\
               next_states.to(self.device), dones.to(self.device)</code></pre><figcaption>Here is a simply replay buffer class, which holds experience tuples of state, action, reward, next_state, and done values. It returns these values in separate tensors in the batch_get function.</figcaption></figure><!--kg-card-end: code--><p>Given a state, the Q network outputs the expected return for each action. So, we'd expect \(Q(s_1)+r_0=Q(s_0)\). This is something we can optimize for. In the target rewards \(Q(s_1)+\gamma r_0\) we use a discount factor called \(\gamma\). By increasing or lowering \(\gamma\) we can prioritize optimizing the current reward \(r_0\) or future rewards \(Q(s_1)\) more.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">self.optimizer = optim.RMSprop(self.local_qnetwork.parameters())
    def qnetwork_step(self):
    	# Get the minibatch of experiences
        states, actions, rewards, next_states, dones = self.replay_buffer.batch_get(self.batch_size, self.state_size)
        
        target_rewards = rewards + self.gamma * torch.max(self.target_qnetwork(next_states), dim=1)[0].unsqueeze(1) * (1 - dones)
        local_rewards = self.local_qnetwork(states).gather(1, actions.long())

        self.optimizer.zero_grad()
        loss = F.mse_loss(local_rewards, target_rewards)
        loss.backward()
        self.optimizer.step()

        return loss.item()</code></pre><figcaption>The agent learns using backpropagation.</figcaption></figure><!--kg-card-end: code--><p>This also brings us back to the second improvement of deep Q networks. Rather than using the Q network which is being updated to also get the target rewards, we use a target Q network which is periodically updated by the local Q network. This has been shown to improve stability. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">    def agent_step(self, state, eps):
        next_state, reward, done = self.env_step(state, eps)
        if len(self.replay_buffer.queue) &lt; self.replay_start_size:
            return next_state, reward, None, done

        # Update the local q network every local_update_frequency steps
        loss = None
        if self.episode_step % self.local_update_frequency == 0:
            loss = self.qnetwork_step()

        # Update the target q network every target_update_frequency steps
        if self.episode_step % self.target_update_frequency == 0:
            self.target_qnetwork.load_state_dict(self.local_qnetwork.state_dict())

        self.episode_step += 1
        return next_state, reward, loss, done

    def env_step(self, state, eps):
    	# Choose an action
        action = self.policy(state, eps)
        # Environment step
        next_state, reward, done, _ = self.env.step(action)

		# Store the experience for later use
        self.replay_buffer.put([state, action, reward, next_state, done])
        return next_state, reward, done</code></pre><figcaption>This ties up the DQN agent class.</figcaption></figure><!--kg-card-end: code--><p>Finally, we need a training loop. Since the agent_step function does most of the heavy lifting, the training loop is fairly small.</p><!--kg-card-begin: code--><pre><code class="language-Python">def train(agent, eps, num_episodes):

    for episode in range(num_episodes):
        state = agent.reset()

        total_reward = 0
        episode_step = 0
        for i in range(500):
            next_state, reward, loss, done = agent.agent_step(state, eps)

            total_reward += reward

            episode_step += 1
            state = next_state
            if done:
                break
                
        # Update epsilon
        eps = max(eps * 0.995, 0.01)      </code></pre><!--kg-card-end: code--><h3 id="results">Results</h3><p>I trained the Q network on <a href="https://gym.openai.com/envs/LunarLander-v2/">OpenAI's Lunar Lander environment</a>, which has a continuous state space of 8 and a discrete action space of 4. In this environment, the agent is reward for landing softly and at a certain location. Having a reward of 200 is considered solved.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2020/02/vanilla_rewards.png" class="kg-image" alt="Deep Reinforcement Learning [1/4]- Deep Q Learning"><figcaption>Graph of total rewards vs. steps. The agent appears to average a total reward of 250.</figcaption></figure><!--kg-card-end: image--><!--kg-card-begin: html--><iframe src="https://giphy.com/embed/QWjkwEO83X8bnINU5i" width="480" height="354" frameborder="0" class="giphy-embed" allowfullscreen></iframe><center><font size="3">An agent learning how to land a spaceship. As the training progresses, the agent learns how to more efficiently use its fuel. </font></center><!--kg-card-end: html--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="improvements">Improvements</h2><h3 id="double-dqn">Double DQN</h3><p>In <a href="https://arxiv.org/pdf/1509.06461.pdf">this paper</a>, the authors introduced a simple improvement to the way the loss is calculated. Instead of calculating target rewards just using the target network, the authors calculated the target rewards by selecting the actions from the local Q network and getting the expected rewards from the target Q network. So the only change we have to make is to change the action selection from the target Q network to the local Q network. This helps remove overestimation of expected rewards.</p><!--kg-card-begin: code--><pre><code class="language-Python"># How we previously calculated the target_rewards
target_rewards = rewards + self.gamma * torch.max(self.target_qnetwork(next_states), dim=1)[0].unsqueeze(1) * (1 - dones)
# Double DQN 
next_target_actions = torch.argmax(self.local_qnetwork(next_states), dim=1).unsqueeze(1)
next_target_rewards = self.target_qnetwork(next_states).gather(1, next_target_actions)
target_rewards = rewards + self.gamma * next_target_rewards * (1 - dones)</code></pre><!--kg-card-end: code--><h3 id="dueling-networks">Dueling Networks</h3><p><a href="https://arxiv.org/pdf/1511.06581.pdf">This paper</a> introduces dueling networks, which alters the Q network behavior. The idea behind dueling networks is instead of just estimating the action-values, it estimates both the state and action values. The agent can now learn which states are valuable regardless of action taken.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2020/02/Screenshot-from-2020-02-29-13-32-40.png" class="kg-image" alt="Deep Reinforcement Learning [1/4]- Deep Q Learning"><figcaption>Source: <a href="https://arxiv.org/pdf/1511.06581.pdf">Dueling Network Architectures for Deep Reinforcement Learning</a></figcaption></figure><!--kg-card-end: image--><p> The Q network now consists of shared layers followed by 2 streams which branch off the shared layers. The value stream estimates the expected value of a state, and it's a single number. The advantage stream estimates how much better the actions are relative to each other. Then the final output of the Q network uses both the values from the value and advantage streams, either \(V+A-\text{max}(A)\) or \(V+A-\text{mean}(A)\).</p><!--kg-card-begin: code--><pre><code class="language-Python">class DQNModel(nn.Module):
    def __init__(self):
        super().__init__()

        self.shared_stream = nn.Sequential(
            nn.Linear(8, 64),
            nn.ReLU()
        )

        self.advantage_stream = nn.Sequential(
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 4),
        )

        self.value_stream = nn.Sequential(
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        # Takes in a state vector with length 8 and outputs an action vector with length 4
        x = self.shared_stream(state)
        advantages = self.advantage_stream(x)
        value = self.value_stream(x)
        return value + advantages - torch.mean(advantages)</code></pre><!--kg-card-end: code--><h3 id="prioritized-experience-replay">Prioritized Experience Replay</h3><p><a href="https://arxiv.org/pdf/1511.05952.pdf">This paper</a> introduces prioritized experience replay. Rather than uniformly sampling from the replay buffer, you prioritize most important experiences. One measure of the importance of experiences is the TD error, which is the absolute value of the difference between target and local rewards. We'd expect that we'd need to learn more from experiences with higher errors. So we can store these TD errors in the experience tuples after we calculate them in the learning step.</p><!--kg-card-begin: code--><pre><code class="language-Python">td_error = (local_rewards - target_rewards.detach()) ** 2

self.replay_buffer.update_priorities(indices, td_error.data.cpu() + 0.0001)</code></pre><!--kg-card-end: code--><p>In the replay buffer class, we calculate the probability of each experience being chosen to be \(\frac{p_i^\alpha}{\sum_{k} p_k^\alpha}\), where \(\alpha\) is a constant designed to control the amount of randomness desired when choosing experiences.</p><!--kg-card-begin: code--><pre><code class="language-Python"># Get the weights for all experiences
probs = priorities ** self.alpha
probs = probs / np.sum(probs)

# Get the weighted experiences
indices = random.choice(np.arange(len(self.queue)), batch_size, p=probs, replace=False)
experiences = [self.queue[i] for i in indices]</code></pre><!--kg-card-end: code--><p>We also have to modify our Q network update step. Because we're sampling from a nonuniform distribution, we have to multiply the TD errors with something called the importance sampling weights, \(w_i=\left(\frac{1}{N \cdot  P(i)}\right)^\beta\). We also can update the priorities for the experiences in the replay buffer. Overall, this makes training faster and more stable.</p><!--kg-card-begin: code--><pre><code>is_weights = (1 / (len(self.queue) * probs[indices])) ** beta
is_weights /= is_weights.max()

td_error = (local_rewards - target_rewards.detach()) ** 2
loss = torch.mean(is_weights.unsqueeze(1) * td_error)
loss.backward()</code></pre><!--kg-card-end: code--><p>The final code is available <a href="https://github.com/andrewpeng02/deeprl-pytorch">here</a>.</p>]]></content:encoded></item><item><title><![CDATA[GPU Monitor IntelliJ Plugin]]></title><description><![CDATA[I made an IntelliJ plugin that monitors GPU vitals that's compatible with PyCharm (my IDE of preference), IntelliJ IDEA, and others. ]]></description><link>https://andrewpeng.dev/gpu-monitor-intellij-plugin/</link><guid isPermaLink="false">5e114ee1f0a1c16c6662f131</guid><dc:creator><![CDATA[Andrew Peng]]></dc:creator><pubDate>Sun, 05 Jan 2020 02:54:44 GMT</pubDate><media:content url="https://andrewpeng.dev/content/images/2020/01/pluginIcon.svg" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2020/01/gpu-monitor-graphs-2.png" class="kg-image" alt="GPU Monitor IntelliJ Plugin"></figure><!--kg-card-end: image--><h2 id="features">Features</h2><img src="https://andrewpeng.dev/content/images/2020/01/pluginIcon.svg" alt="GPU Monitor IntelliJ Plugin"><p>I made an IntelliJ plugin that monitors GPU vitals that's compatible with PyCharm (my IDE of preference), IntelliJ IDEA, and others. It monitors GPU temperature, GPU usage, and memory usage and plots the graphs using <a href="http://www.jfree.org/jfreechart/">JFreeChart</a>. It's useful for keeping track of memory utilization, etc while training my machine learning models.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2020/01/gpu-monitor-config.png" class="kg-image" alt="GPU Monitor IntelliJ Plugin"></figure><!--kg-card-end: image--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="compatibility">Compatibility</h2><p>It currently only supports single nvidia gpu setups, but I can add more support if needed.  </p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><p>The plugin is located <a href="https://plugins.jetbrains.com/plugin/13597-gpu-monitor">here</a>, and the source code is located <a href="https://github.com/andrewpeng02/gpu-monitor-plugin">here</a>. Please add feature requests and/or bugs in the GitHub repository!</p>]]></content:encoded></item><item><title><![CDATA[Transformer [2/2]- PytorchTransformers Library]]></title><description><![CDATA[In part 2 of my post, I'm going to go over huggingface's pytorch transformers library.]]></description><link>https://andrewpeng.dev/transformer-huggingface/</link><guid isPermaLink="false">5ddade70b2078d1be349f35f</guid><dc:creator><![CDATA[Andrew Peng]]></dc:creator><pubDate>Tue, 26 Nov 2019 03:12:41 GMT</pubDate><media:content url="https://andrewpeng.dev/content/images/2019/11/transformers_logo_name-3.png" medium="image"/><content:encoded><![CDATA[<img src="https://andrewpeng.dev/content/images/2019/11/transformers_logo_name-3.png" alt="Transformer [2/2]- PytorchTransformers Library"><p>In part 2 of my post, I'm going to go over huggingface's pytorch transformers library located <a href="https://github.com/huggingface/transformers">here</a>. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/transformers_logo_name-1.png" class="kg-image" alt="Transformer [2/2]- PytorchTransformers Library"></figure><!--kg-card-end: image--><p>This library provides over 30 pretrained state of the art transformer models on a variety of languages. Alike convolutional neural networks, transformers trained on a different linguistic dataset can be easily retrained on different language datasets. Using pretrained models rather than starting from scratch greatly reduces training time and can sometimes increase accuracy over training a model from scratch. </p><p>In this tutorial, I'll be fine-tuning a DistilBert model to predict the sentiment of IMDB movie reviews. DistilBert is a smaller version of the BERT model, allowing it to get most of the performance of BERT for much less training. More details are located in huggingface's <a href="https://medium.com/huggingface/distilbert-8cf3380435b5">blog post</a>. </p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="implementation">Implementation</h2><h3 id="data-preprocessing">Data Preprocessing</h3><p>I'm using an <a href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews">IMDB movie reviews dataset</a>, which has a list of movie reviews as well as either a "positive" or "negative" sentiment. </p><p>To use huggingface's pretrained models, we have to use their provided tokenizer. Because acquiring the sentiment from a review isn't reliant on stop words such as 'and', 'or', or 'at', we remove them. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">from bs4 import BeautifulSoup
from transformers import DistilBertTokenizer

from nltk.corpus import stopwords

stopwords = stopwords.words('english')
distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

review = BeautifulSoup(review, "html.parser").get_text()
review = review.lower()
review = distilbert_tokenizer.tokenize(review)

# Remove stopwords/punctuation
review = [w for w in review if w not in stopwords and contains_alphabet(w)]</code></pre><figcaption>Preprocess data class</figcaption></figure><!--kg-card-end: code--><p>Now in the dataset class, we attach the start token [CLF], insert padding, and convert the tokenized words to indicies. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">seq_length = 256
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

review = ['[CLF]'] + review[:seq_length - 1]
review = review + ['[PAD]' for _ in range(self.seq_length - len(review))]
review = tokenizer.convert_tokens_to_ids(review)</code></pre><figcaption>Dataset class</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h3 id="training">Training</h3><p>Instantiating the DistilBert model is as simple as importing the class. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2).to('cuda')
lrs = [{'params': model.distilbert.parameters(), 'lr': kwargs['lr_transformer']},
           {'params': model.pre_classifier.parameters()},
           {'params': model.classifier.parameters()}]
optim = Adam(lrs, lr=kwargs['lr_classifier'], eps=kwargs['eps'])</code></pre><figcaption>Train class. It may take a while to download the pretrained model. Here, I apply different learning rates to the transformer and classifier to achieve better results.&nbsp;</figcaption></figure><!--kg-card-end: code--><p>To train the model, all we have to do is pass the reviews and labels to the model and we get our losses back!</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python ">reviews = reviews.to('cuda')
labels = labels.to('cuda').long()

optim.zero_grad()
loss = model(reviews, labels=labels)[0]

loss.backward()
optim.step()</code></pre><figcaption>Train class. After I get the losses from every minibatch, I backpropagate.</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="results">Results</h2><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/Steps-and-Losses.png" class="kg-image" alt="Transformer [2/2]- PytorchTransformers Library"></figure><!--kg-card-end: image--><p>After 3 epochs, the pretrained transformer reaches a validation loss of 0.262 and a validation accuracy of 0.9021. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/Model-and-Validation-Accuracy.png" class="kg-image" alt="Transformer [2/2]- PytorchTransformers Library"></figure><!--kg-card-end: image--><p>I trained other common linguistic models including an LSTM and a transformer implemented using nn.Transformer. As shown in the graph, the huggingface transformer still edges out in terms of accuracy.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/Model-and-Training-time-per-epoch.png" class="kg-image" alt="Transformer [2/2]- PytorchTransformers Library"></figure><!--kg-card-end: image--><p></p><p>DistilBert took the longest time to train by far with approximately — min, likely because it has the most amount of parameters. This is followed by nn.Transformer and then by the LSTM. </p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="further-research">Further Research</h2><p>Some further improvements that could be made to this model:</p><ol><li>Improving the dataloader by allowing for variable length batches in order to reduce the amount of wasted memory spent on padding</li><li>Optimizing the parameters further, such as by adding differential learning rates</li></ol><p>My code is located <a href="https://github.com/andrewpeng02/imdb-sentiment">here</a>. </p>]]></content:encoded></item><item><title><![CDATA[Transformer [1/2]- Pytorch's nn.Transformer]]></title><description><![CDATA[In part 1 of my series on transformers, I'm going to go over implementing a neural machine translation model using Pytorch's new nn.Transformer module. ]]></description><link>https://andrewpeng.dev/transformer-pytorch/</link><guid isPermaLink="false">5dc856062817c216af7800ec</guid><dc:creator><![CDATA[Andrew Peng]]></dc:creator><pubDate>Sun, 10 Nov 2019 20:11:25 GMT</pubDate><media:content url="https://andrewpeng.dev/content/images/2019/11/encoder-3-1.png" medium="image"/><content:encoded><![CDATA[<img src="https://andrewpeng.dev/content/images/2019/11/encoder-3-1.png" alt="Transformer [1/2]- Pytorch's nn.Transformer"><p>In part 1 of my series on transformers, I'm going to go over implementing a neural machine translation model using Pytorch's new nn.Transformer module. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/encoder-3.png" class="kg-image" alt="Transformer [1/2]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>Transformers, introduced by the paper <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need,</a> inherently don't have a sense of time, They instead rely on <strong>positional encoding </strong>to encode the order of elements. This gives the transformer architecture an important advantage over other language models such as recurrent neural networks: they are parallelizable and easy to expand. This has allowed huge models such as the 1.5 billion parameter <a href="https://openai.com/blog/better-language-models/">GPT-2</a> to achieve state of the art performance on language modelling. </p><h2 id="pytorch">Pytorch</h2><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/image.png" class="kg-image" alt="Transformer [1/2]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>Now, with the release of Pytorch 1.2, we can build transformers in pytorch! We'll go over the basics of the transformer architecture and how to use nn.Transformer. In a transformer, the input sentence goes through an encoder where the sentence gets passed through encoders to become memory. Then the output sentence and memory passes through decoders where it outputs the translated sentence. </p><h3 id="the-encoder">The Encoder</h3><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/encoder-1.png" class="kg-image" alt="Transformer [1/2]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>First, we tokenize the input data, pad the array if necessary, and convert the tokens to embeddings. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">import spacy

# Tokenize sentence
lang_model = spacy.load('en', disable=['tagger', 'parser', 'ner'])
sentence = sentence.lower()
sentence = [tok.text for tok in lang_model.tokenizer(sentence) if tok.text not in punctuation]

# Create a dictionary which maps tokens to indices (train contains all the training sentences)
freq_list = Counter()
    for sentence in train:
        freq_list.update(sentence)

# Convert tokens to indices
indices = [freq_list[word] for word in sentence if word in freq_list]</code></pre><figcaption>Here, I tokenize the sentence using spacy and convert the sentence to indices</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">import torch
from einops import rearrange

self.embed_src = nn.Embedding(vocab_size, d_model)
src = rearrange(indices, 'n s -&gt; s n')
src = self.embed_src(src)</code></pre><figcaption>In the LanguageTransformer class, I create an embedding and embed the batch</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/encoder-2.png" class="kg-image" alt="Transformer [1/2]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>Now we add the positional encoding to the sentences in order to give some order to the words. In the Attention is All You Need model, they use sine and cosine embeddings to give generalizability to longer sentence sizes. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">import math 
self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)
src = self.pos_enc(src * math.sqrt(self.d_model))</code></pre><figcaption>In the LanguageTransformer class, I scale src in order to reduce variance then apply the positional encoding</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python"># Source: https://pytorch.org/tutorials/beginner/transformer_tutorial
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=100):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_ter
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)</code></pre><figcaption>PositionalEncoding class</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/encoder-3-2.png" class="kg-image" alt="Transformer [1/2]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>Masking in the encoder is required to make sure any padding doesn't contribute to the self-attention mechanism. In Pytorch, this is done by passing src_key_padding_mask to the transformer. For the example, this looks like [False, False, False, False, False, False, False, True, True, True] where the True positions should be masked. The output of the encoder is called memory. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">for i, sentence in enumerate(batch):
        masks.append([False for _ in range(len(sentence))] + [True for _ in range(seq_length - len(sentence))])
        batch[i] = sentence + [0 for _ in range(seq_length - len(sentence))]</code></pre><figcaption>Padding and masking is taken care of in the dataset class</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h3 id="the-decoder">The Decoder</h3><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/decoder-1.png" class="kg-image" alt="Transformer [1/2]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>Now we can move onto the decoder architecture. The initial steps are very similar to that of the encoder. We embed and pass all but the very last token of each sentence into the decoders. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">self.embed_tgt = nn.Embedding(vocab_size, d_model)

tgt_inp = tgt[:, :-1]
tgt = rearrange(tgt_inp, 'n t -&gt; t n')
tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))</code></pre><figcaption>In the LanguageTransformer class, we embed and encode the target sequence</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/decoder-2.png" class="kg-image" alt="Transformer [1/2]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>We then pass these sequences through m decoders.  In each decoder, the sequences propagate through self attention and then attention with the memory (from the encoder). So the decoder requires 3 masks: </p><ol><li>tgt_mask: Used in the self-attention, it ensures the decoder doesn't look at future tokens from a given subsequence. This looks like [[0 -inf -inf ... ], [0 0 -inf ...] ... [0 0 0 ...]]</li><li>tgt_key_padding_mask: Also used in the self-attention, it ensures that the padding in the target sequence isn't accounted for. </li><li>memory_key_padding_mask: Used in the attention with the memory, it ensures the padding in the memory isn't used. This is the same as the src_key_padding_mask</li></ol><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">def gen_nopeek_mask(length):
    mask = rearrange(torch.triu(torch.ones(length, length)) == 1, 'h w -&gt; w h')
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))

    return mask

memory_key_padding_mask = src_key_padding_mask.clone()
tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to('cuda')</code></pre><figcaption>This is in the train method. src_key_padding_mask and tgt_key_padding_mask is expected from the dataloader</figcaption></figure><!--kg-card-end: code--><p>Afterwards, we pass each of the output sequences through a fully connected layer that outputs a probability for each token in the vocab size. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)

output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,
                                  tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)
output = rearrange(output, 't n e -&gt; n t e')
output = self.fc(output)</code></pre><figcaption>This is in the LanguageTransformer class</figcaption></figure><!--kg-card-end: code--><p>And here is the completed LanguageTransformer class!</p><!--kg-card-begin: code--><pre><code class="language-Python">class LanguageTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos_dropout, trans_dropout):
        super().__init__()
        self.d_model = d_model
        self.embed_src = nn.Embedding(vocab_size, d_model)
        self.embed_tgt = nn.Embedding(vocab_size, d_model)
        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)

        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)
        self.fc = nn.Linear(d_model, vocab_size)

    def forward(self, src, tgt, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_mask):
        src = rearrange(src, 'n s -&gt; s n')
        tgt = rearrange(tgt, 'n t -&gt; t n')
        src = self.pos_enc(self.embed_src(src) * math.sqrt(self.d_model))
        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))

        output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,
                                  tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)
        output = rearrange(output, 't n e -&gt; n t e')
        return self.fc(output)</code></pre><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h3 id="results">Results</h3><p>I used the tatoeba dataset, a small dataset with around 160000 english to french language pairs available <a href="http://www.manythings.org/anki/">here</a>. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/11/data.png" class="kg-image" alt="Transformer [1/2]- Pytorch's nn.Transformer"><figcaption>A relatively small dataset with short sentences</figcaption></figure><!--kg-card-end: image--><p>Here are the results of training for 20 epochs:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/11/Losses-and-Time.png" class="kg-image" alt="Transformer [1/2]- Pytorch's nn.Transformer"></figure><!--kg-card-end: image--><p>My model achieves a validation loss of 0.99. However, it starts overfitting around epoch 15 based from the validation loss being higher than the train loss. And finally, some results of translating sentences:</p><!--kg-card-begin: markdown--><p>I am giving you a gift.: Je vous donne un cadeau.<br>
How did you find that?: Comment l'as-tu trouvée?<br>
I'm going to run to your house.: Je vais courir à votre maison.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="further-research">Further Research</h2><p>Some improvements that could be made:</p><ol><li>Using beam search to translate sentences</li><li>Running the model on larger datasets </li><li>Using torchtext instead of hacking my own dataset class to get more consistent batches</li><li>Using smoothened loss</li></ol><p>My code is located <a href="https://github.com/andrewpeng02/transformer-translation">here</a>. </p>]]></content:encoded></item><item><title><![CDATA[College Tuition Prediction [2/2]- Model]]></title><description><![CDATA[In part 2, we're building the model to predict college tuition.]]></description><link>https://andrewpeng.dev/college-tuition-prediction-2/</link><guid isPermaLink="false">5d5a0776e0af2b6f96d80f7a</guid><dc:creator><![CDATA[Andrew Peng]]></dc:creator><pubDate>Mon, 19 Aug 2019 16:57:13 GMT</pubDate><media:content url="https://andrewpeng.dev/content/images/2019/08/ancient-architecture-bricks-220311.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://andrewpeng.dev/content/images/2019/08/ancient-architecture-bricks-220311.jpg" alt="College Tuition Prediction [2/2]- Model"><p>Now that I've finished preparing the data, it's time to build the model!</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/ancient-architecture-bricks-220311-1.jpg" class="kg-image" alt="College Tuition Prediction [2/2]- Model"></figure><!--kg-card-end: image--><p>I chose to try sklearn's support vector machine and random forest, as well as xgboost because predicting college tuition should be a relatively simple task. To find the best model, I used sklearn's GridSearchCV to brute force the parameters for each model. GridSearchCV takes the model (in this case a support vector regressor), a parameter grid (7*5*7=245 total combinations), a scoring method (regression so I'll use mean squared error), and number of folds for cross validation.</p><!--kg-card-begin: code--><pre><code class="language-python">model = svm.SVR()
grid_values = {'gamma': [0.01, 0.03, 0.1, 0.3, 1, 3, 9], 
               'C': [0.1, 0.3, 1, 3, 9], 
               'epsilon': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1]}
grid_search = GridSearchCV(model, param_grid=grid_values, n_jobs=-1, scoring='neg_mean_squared_error', cv=3)
grid_search.fit(X_train, y_train)</code></pre><!--kg-card-end: code--><p>Using GridSearchCV is similar for a random forest regressor and xgboost regressor, but it takes a different parameter grid. Here are the results:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/results.png" class="kg-image" alt="College Tuition Prediction [2/2]- Model"></figure><!--kg-card-end: image--><p>Unsurprisingly, the xgboost model did the best, followed by random forest and SVR. Here is a scatterplot of the xgboost model predicting on the test set:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/predictions.png" class="kg-image" alt="College Tuition Prediction [2/2]- Model"></figure><!--kg-card-end: image--><p>Visualizing the plot confirmed my hypothesis that the model would do best at lower tuition amounts. With xgboost, I can retrieve the feature importance and plot it as well.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/feature_importance.png" class="kg-image" alt="College Tuition Prediction [2/2]- Model"></figure><!--kg-card-end: image--><p>Admissions yield and professor salary contributed the most, while my categorical features regarding college size and location provided little predictive power.</p><p>Full code is located over on my <a href="https://github.com/andrewpeng02/college-tuition/tree/master">GitHub</a>.</p>]]></content:encoded></item><item><title><![CDATA[College Tuition Prediction [1/2]- Data Preparation]]></title><description><![CDATA[Welcome to part 1 to a series of posts regarding my college tuition project, where I first prepare and visualize the data.]]></description><link>https://andrewpeng.dev/college-tuition-prediction-1/</link><guid isPermaLink="false">5d5a0730e0af2b6f96d80f71</guid><dc:creator><![CDATA[Andrew Peng]]></dc:creator><pubDate>Mon, 19 Aug 2019 02:20:00 GMT</pubDate><media:content url="https://andrewpeng.dev/content/images/2019/08/delfi-de-la-rua-A_InfAQM_lU-unsplash-2.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://andrewpeng.dev/content/images/2019/08/delfi-de-la-rua-A_InfAQM_lU-unsplash-2.jpg" alt="College Tuition Prediction [1/2]- Data Preparation"><p>Welcome to part 1 to a series of posts regarding my college tuition project!</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/delfi-de-la-rua-A_InfAQM_lU-unsplash-1.jpg" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"></figure><!--kg-card-end: image--><p>In this project, I predict the tuition of colleges around the US using data from the <a href="https://nces.ed.gov/ipeds/use-the-data">National Center for Education Statistics</a>. I chose features I thought would best predict college tuition, such as admission rate, graduation rate, and college location, and I ended up with roughly 20 features.</p><h3 id="data-visualization">Data Visualization</h3><p>Always the first process you should do with your data is to visualize it. Here, I created various plots of tuition vs. various factors I thought would be the best predictors using Seaborn.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/08/tuition_hist.png" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"><figcaption>Histogram of Tuition</figcaption></figure><!--kg-card-end: image--><p>From this graph, I can tell that tuition is right skewed with peak of around 8000$.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/08/tuition_admissions.png" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"><figcaption>Scatterplot of Tuition and Admissions Rate</figcaption></figure><!--kg-card-end: image--><p>While admissions rate seems much more varied from 0-20000$, the rate appears to generally decrease.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/08/tuition_grad_rate.png" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"><figcaption>Scatterplot of Tuition and Graduation Rate</figcaption></figure><!--kg-card-end: image--><p>Again, a similar trend where graduation rate varies a lot from 0-20000$ but exhibits an increase afterwards.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/08/tuition_salary.png" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"><figcaption>Scatterplot of Tuition and Professor Salary</figcaption></figure><!--kg-card-end: image--><p>As expected, as tuition increases, professors' salaries also increases. From these graphs, I predict that my models will be able to predict college tuition more accurately around 30,000$ because of its lower variability in factors like graduation rate.</p><h3 id="data-preparation">Data Preparation</h3><p>The data.csv file contains the colleges as rows and features (including tuition) as columns. I will be using Pandas for loading data and scikit-learn for preprocessing.</p><!--kg-card-begin: code--><pre><code class="language-python">import pandas as pd
import joblib

import sklearn.preprocessing as preprocessing
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer</code></pre><!--kg-card-end: code--><p>I removed the colleges that didn't have tuition filled in, as well as the rows which have less than 18 out of the 25 features so that there are enough features to predict tuition. This left about 2900 colleges from the original total of 7000.</p><!--kg-card-begin: code--><pre><code class="language-python">data = pd.read_csv('data/data.csv')
data = data[pd.notnull(data['Tuition and fees'])]

min_fields = 18
for index, row in data.iterrows():
    filled = 0

    for name, field in row.items():
        if str(field) != 'nan':
            filled += 1

    if filled &lt;= min_fields:
        data.drop(index, inplace=True)
        data.drop(data.columns[[0, 1, 2]], axis=1, inplace=True)</code></pre><!--kg-card-end: code--><p>Split data into train and test sets using train_test_split() with 80% of the data going into the train set.</p><!--kg-card-begin: code--><pre><code>data = data.reset_index(drop=True)
train, test = train_test_split(data, test_size=0.2)
X_train, y_train = train.drop(train.columns[[2]], axis=1), train.iloc[:,2]
X_test, y_test = test.drop(test.columns[[2]], axis=1), test.iloc[:, 2]</code></pre><!--kg-card-end: code--><p>Scale targets between 0 and 1 for faster convergence. While the sklearn libary contains a plethora of scalers, I chose min max scaler because of its simplicity and skewed dataset. I saved the scaler using joblib so I could later scale the predicted targets back.</p><!--kg-card-begin: code--><pre><code class="language-python">scaler = preprocessing.MinMaxScaler()
scaler.fit(y_train.values.reshape(-1, 1))
y_train, y_test = scaler.transform(y_train.values.reshape(-1, 1)), scaler.transform(y_test.values.reshape(-1, 1))
joblib.dump(scaler, 'min_max_scaler.pkl')
</code></pre><!--kg-card-end: code--><p>To encode the numeric features, I defined the columns which had numeric features and a Pipeline from sklearn. I first imputed the data (meaning I replaced missing values with the mean) and then scaled the data. For the categorical features, I instead used a OneHotEncoder in a Pipeline to convert the categories to numeric values.</p><!--kg-card-begin: code--><pre><code class="language-python">numeric_features = X_train.columns[[0, 1] + [i for i in range(4, 24)]]
numeric_trans = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),
('scaler', preprocessing.MinMaxScaler())])

column_features = X_train.columns[[2, 3]]
column_trans = Pipeline(steps=[('encoder', preprocessing.OneHotEncoder(drop='first'))])</code></pre><!--kg-card-end: code--><p>I then created a ColumnTransformer by passing the pipelines and feature names and then transformed the data.</p><!--kg-card-begin: code--><pre><code>transformer = ColumnTransformer(transformers=[('numeric', numeric_trans, numeric_features),
('categorical', column_trans, column_features)],
remainder='passthrough')
transformer.fit(X_train)

X_train = pd.DataFrame(transformer.transform(X_train))
X_test = pd.DataFrame(transformer.transform(X_test))</code></pre><!--kg-card-end: code--><p>Now that the data is ready, it's finally time for creating the model in <a href="https://andrewpeng.dev/college-tuition-prediction-1/andrewpeng.dev/college-tuition-prediction-2">Part 2</a>!</p><p><br></p><p>Full code is located over on my <a href="https://github.com/andrewpeng02/college-tuition/tree/master">GitHub</a>.</p>]]></content:encoded></item></channel></rss>