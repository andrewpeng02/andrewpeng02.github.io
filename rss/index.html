<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Andrew Peng]]></title><description><![CDATA[Andrew Peng's Blog]]></description><link>https://andrewpeng.dev/</link><image><url>https://andrewpeng.dev/favicon.png</url><title>Andrew Peng</title><link>https://andrewpeng.dev/</link></image><generator>Ghost 2.28</generator><lastBuildDate>Tue, 20 Aug 2019 20:54:34 GMT</lastBuildDate><atom:link href="https://andrewpeng.dev/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[College Tuition Prediction [2/2]- Model]]></title><description><![CDATA[<p>Now that I've finished preparing the data, it's time to build the model!</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/ancient-architecture-bricks-220311-1.jpg" class="kg-image"></figure><!--kg-card-end: image--><p>I chose to try sklearn's support vector machine and random forest, as well as xgboost because predicting college tuition should be a relatively simple task. To find the best model, I used sklearn's GridSearchCV to brute force</p>]]></description><link>https://andrewpeng.dev/college-tuition-prediction-2/</link><guid isPermaLink="false">5d5a0776e0af2b6f96d80f7a</guid><dc:creator><![CDATA[Andrew Peng]]></dc:creator><pubDate>Mon, 19 Aug 2019 16:57:13 GMT</pubDate><media:content url="https://andrewpeng.dev/content/images/2019/08/ancient-architecture-bricks-220311.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://andrewpeng.dev/content/images/2019/08/ancient-architecture-bricks-220311.jpg" alt="College Tuition Prediction [2/2]- Model"><p>Now that I've finished preparing the data, it's time to build the model!</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/ancient-architecture-bricks-220311-1.jpg" class="kg-image" alt="College Tuition Prediction [2/2]- Model"></figure><!--kg-card-end: image--><p>I chose to try sklearn's support vector machine and random forest, as well as xgboost because predicting college tuition should be a relatively simple task. To find the best model, I used sklearn's GridSearchCV to brute force the parameters for each model. GridSearchCV takes the model (in this case a support vector regressor), a parameter grid (7*5*7=245 total combinations), a scoring method (regression so I'll use mean squared error), and number of folds for cross validation.</p><!--kg-card-begin: code--><pre><code class="language-python">model = svm.SVR()
grid_values = {'gamma': [0.01, 0.03, 0.1, 0.3, 1, 3, 9], 
               'C': [0.1, 0.3, 1, 3, 9], 
               'epsilon': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1]}
grid_search = GridSearchCV(model, param_grid=grid_values, n_jobs=-1, scoring='neg_mean_squared_error', cv=3)
grid_search.fit(X_train, y_train)</code></pre><!--kg-card-end: code--><p>Using GridSearchCV is similar for a random forest regressor and xgboost regressor, but it takes a different parameter grid. Here are the results:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/results.png" class="kg-image" alt="College Tuition Prediction [2/2]- Model"></figure><!--kg-card-end: image--><p>Unsurprisingly, the xgboost model did the best, followed by random forest and SVR. Here is a scatterplot of the xgboost model predicting on the test set:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/predictions.png" class="kg-image" alt="College Tuition Prediction [2/2]- Model"></figure><!--kg-card-end: image--><p>Visualizing the plot confirmed my hypothesis that the model would do best at lower tuition amounts. With xgboost, I can retrieve the feature importance and plot it as well.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/feature_importance.png" class="kg-image" alt="College Tuition Prediction [2/2]- Model"></figure><!--kg-card-end: image--><p>Admissions yield and professor salary contributed the most, while my categorical features regarding college size and location provided little predictive power.</p><p>Full code is located over on my <a href="https://github.com/andrewpeng02/college-tuition/tree/master">GitHub</a>.</p>]]></content:encoded></item><item><title><![CDATA[College Tuition Prediction [1/2]- Data Preparation]]></title><description><![CDATA[<p>Welcome to part 1 to a series of posts regarding my college tuition project!</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/delfi-de-la-rua-A_InfAQM_lU-unsplash-1.jpg" class="kg-image"></figure><!--kg-card-end: image--><p>In this project, I predict the tuition of colleges around the US using data from the <a href="https://nces.ed.gov/ipeds/use-the-data">National Center for Education Statistics</a>. I chose features I thought would best predict college tuition, such as admission rate, graduation</p>]]></description><link>https://andrewpeng.dev/college-tuition-prediction-1/</link><guid isPermaLink="false">5d5a0730e0af2b6f96d80f71</guid><dc:creator><![CDATA[Andrew Peng]]></dc:creator><pubDate>Mon, 19 Aug 2019 02:20:00 GMT</pubDate><media:content url="https://andrewpeng.dev/content/images/2019/08/delfi-de-la-rua-A_InfAQM_lU-unsplash-2.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://andrewpeng.dev/content/images/2019/08/delfi-de-la-rua-A_InfAQM_lU-unsplash-2.jpg" alt="College Tuition Prediction [1/2]- Data Preparation"><p>Welcome to part 1 to a series of posts regarding my college tuition project!</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://andrewpeng.dev/content/images/2019/08/delfi-de-la-rua-A_InfAQM_lU-unsplash-1.jpg" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"></figure><!--kg-card-end: image--><p>In this project, I predict the tuition of colleges around the US using data from the <a href="https://nces.ed.gov/ipeds/use-the-data">National Center for Education Statistics</a>. I chose features I thought would best predict college tuition, such as admission rate, graduation rate, and college location, and I ended up with roughly 20 features.</p><h3 id="data-visualization">Data Visualization</h3><p>Always the first process you should do with your data is to visualize it. Here, I created various plots of tuition vs. various factors I thought would be the best predictors using Seaborn.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/08/tuition_hist.png" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"><figcaption>Histogram of Tuition</figcaption></figure><!--kg-card-end: image--><p>From this graph, I can tell that tuition is right skewed with peak of around 8000$.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/08/tuition_admissions.png" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"><figcaption>Scatterplot of Tuition and Admissions Rate</figcaption></figure><!--kg-card-end: image--><p>While admissions rate seems much more varied from 0-20000$, the rate appears to generally decrease.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/08/tuition_grad_rate.png" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"><figcaption>Scatterplot of Tuition and Graduation Rate</figcaption></figure><!--kg-card-end: image--><p>Again, a similar trend where graduation rate varies a lot from 0-20000$ but exhibits an increase afterwards.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://andrewpeng.dev/content/images/2019/08/tuition_salary.png" class="kg-image" alt="College Tuition Prediction [1/2]- Data Preparation"><figcaption>Scatterplot of Tuition and Professor Salary</figcaption></figure><!--kg-card-end: image--><p>As expected, as tuition increases, professors' salaries also increases. From these graphs, I predict that my models will be able to predict college tuition more accurately around 30,000$ because of its lower variability in factors like graduation rate.</p><h3 id="data-preparation">Data Preparation</h3><p>The data.csv file contains the colleges as rows and features (including tuition) as columns. I will be using Pandas for loading data and scikit-learn for preprocessing.</p><!--kg-card-begin: code--><pre><code class="language-python">import pandas as pd
import joblib

import sklearn.preprocessing as preprocessing
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer</code></pre><!--kg-card-end: code--><p>I removed the colleges that didn't have tuition filled in, as well as the rows which have less than 18 out of the 25 features so that there are enough features to predict tuition. This left about 2900 colleges from the original total of 7000.</p><!--kg-card-begin: code--><pre><code class="language-python">data = pd.read_csv('data/data.csv')
data = data[pd.notnull(data['Tuition and fees'])]

min_fields = 18
for index, row in data.iterrows():
    filled = 0

    for name, field in row.items():
        if str(field) != 'nan':
            filled += 1

    if filled &lt;= min_fields:
        data.drop(index, inplace=True)
        data.drop(data.columns[[0, 1, 2]], axis=1, inplace=True)</code></pre><!--kg-card-end: code--><p>Split data into train and test sets using train_test_split() with 80% of the data going into the train set.</p><!--kg-card-begin: code--><pre><code>data = data.reset_index(drop=True)
train, test = train_test_split(data, test_size=0.2)
X_train, y_train = train.drop(train.columns[[2]], axis=1), train.iloc[:,2]
X_test, y_test = test.drop(test.columns[[2]], axis=1), test.iloc[:, 2]</code></pre><!--kg-card-end: code--><p>Scale targets between 0 and 1 for faster convergence. While the sklearn libary contains a plethora of scalers, I chose min max scaler because of its simplicity and skewed dataset. I saved the scaler using joblib so I could later scale the predicted targets back.</p><!--kg-card-begin: code--><pre><code class="language-python">scaler = preprocessing.MinMaxScaler()
scaler.fit(y_train.values.reshape(-1, 1))
y_train, y_test = scaler.transform(y_train.values.reshape(-1, 1)), scaler.transform(y_test.values.reshape(-1, 1))
joblib.dump(scaler, 'min_max_scaler.pkl')
</code></pre><!--kg-card-end: code--><p>To encode the numeric features, I defined the columns which had numeric features and a Pipeline from sklearn. I first imputed the data (meaning I replaced missing values with the mean) and then scaled the data. For the categorical features, I instead used a OneHotEncoder in a Pipeline to convert the categories to numeric values.</p><!--kg-card-begin: code--><pre><code class="language-python">numeric_features = X_train.columns[[0, 1] + [i for i in range(4, 24)]]
numeric_trans = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),
('scaler', preprocessing.MinMaxScaler())])

column_features = X_train.columns[[2, 3]]
column_trans = Pipeline(steps=[('encoder', preprocessing.OneHotEncoder(drop='first'))])</code></pre><!--kg-card-end: code--><p>I then created a ColumnTransformer by passing the pipelines and feature names and then transformed the data.</p><!--kg-card-begin: code--><pre><code>transformer = ColumnTransformer(transformers=[('numeric', numeric_trans, numeric_features),
('categorical', column_trans, column_features)],
remainder='passthrough')
transformer.fit(X_train)

X_train = pd.DataFrame(transformer.transform(X_train))
X_test = pd.DataFrame(transformer.transform(X_test))</code></pre><!--kg-card-end: code--><p>Now that the data is ready, it's finally time for creating the model in <a href="https://andrewpeng.dev/college-tuition-prediction-1/andrewpeng.dev/college-tuition-prediction-2">Part 2</a>!</p><p><br></p><p>Full code is located over on my <a href="https://github.com/andrewpeng02/college-tuition/tree/master">GitHub</a>.</p>]]></content:encoded></item></channel></rss>