<!DOCTYPE html>
<html ⚡>
<head>
    <meta charset="utf-8">

    <title>Transformer [1/3]- Pytorch&#x27;s nn.Transformer</title>

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <link rel="shortcut icon" href="../../favicon.ico" type="image/x-icon" />
    <link rel="canonical" href="../" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="Andrew Peng" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Transformer [1/3]- Pytorch&#x27;s nn.Transformer" />
    <meta property="og:description" content="In part 1 of my series on transformers, I&#x27;m going to go over implementing a neural machine translation model using Pytorch&#x27;s new nn.Transformer module. " />
    <meta property="og:url" content="https://andrewpeng.dev/transformer-pytorch/" />
    <meta property="og:image" content="https://andrewpeng.dev/content/images/2019/11/encoder-3-1.png" />
    <meta property="article:published_time" content="2019-11-10T20:11:25.000Z" />
    <meta property="article:modified_time" content="2020-01-17T21:47:33.000Z" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Transformer [1/3]- Pytorch&#x27;s nn.Transformer" />
    <meta name="twitter:description" content="In part 1 of my series on transformers, I&#x27;m going to go over implementing a neural machine translation model using Pytorch&#x27;s new nn.Transformer module. " />
    <meta name="twitter:url" content="https://andrewpeng.dev/transformer-pytorch/" />
    <meta name="twitter:image" content="https://andrewpeng.dev/content/images/2019/11/encoder-3-1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Andrew Peng" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="1125" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Andrew Peng",
        "logo": "https://static.ghost.org/v1.0.0/images/ghost-logo.svg"
    },
    "author": {
        "@type": "Person",
        "name": "Andrew Peng",
        "url": "https://andrewpeng.dev/author/andrew/",
        "sameAs": []
    },
    "headline": "Transformer [1/3]- Pytorch&#x27;s nn.Transformer",
    "url": "https://andrewpeng.dev/transformer-pytorch/",
    "datePublished": "2019-11-10T20:11:25.000Z",
    "dateModified": "2020-01-17T21:47:33.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://andrewpeng.dev/content/images/2019/11/encoder-3-1.png",
        "width": 2000,
        "height": 1125
    },
    "description": "In part 1 of my series on transformers, I&#x27;m going to go over implementing a neural machine translation model using Pytorch&#x27;s new nn.Transformer module. ",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://andrewpeng.dev/"
    }
}
    </script>

    <meta name="generator" content="Ghost 2.28" />
    <link rel="alternate" type="application/rss+xml" title="Andrew Peng" href="../../rss/" />

    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,600,400" />
    <style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{margin:0.67em 0;font-size:2em}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{position:relative;vertical-align:baseline;font-size:75%;line-height:0}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}amp-img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;color:inherit;font:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{cursor:pointer;-webkit-appearance:button}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{margin:0 2px;padding:0.35em 0.625em 0.75em;border:1px solid #c0c0c0}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}html{max-height:100%;height:100%;font-size:62.5%;-webkit-tap-highlight-color:rgba(0, 0, 0, 0)}body{max-height:100%;height:100%;color:#3a4145;background:#f4f8fb;letter-spacing:0.01rem;font-family:"Merriweather", serif;font-size:1.8rem;line-height:1.75em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"kern" 1;-moz-font-feature-settings:"kern" 1;-o-font-feature-settings:"kern" 1}::-moz-selection{background:#d6edff}::selection{background:#d6edff}h1,h2,h3,h4,h5,h6{margin:0 0 0.3em 0;color:#2e2e2e;font-family:"Open Sans", sans-serif;line-height:1.15em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-moz-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-o-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1}h1{text-indent:-2px;letter-spacing:-1px;font-size:2.6rem}h2{letter-spacing:0;font-size:2.4rem}h3{letter-spacing:-0.6px;font-size:2.1rem}h4{font-size:1.9rem}h5{font-size:1.8rem}h6{font-size:1.8rem}a{color:#4a4a4a}a:hover{color:#111}p,ul,ol,dl{margin:0 0 2.5rem 0;font-size:1.5rem;text-rendering:geometricPrecision;-webkit-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-moz-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-o-font-feature-settings:"liga" 1, "onum" 1, "kern" 1}ol,ul{padding-left:2em}ol ol,ul ul,ul ol,ol ul{margin:0 0 0.4em 0;padding-left:2em}dl dt{float:left;clear:left;overflow:hidden;margin-bottom:1em;width:180px;text-align:right;text-overflow:ellipsis;white-space:nowrap;font-weight:700}dl dd{margin-bottom:1em;margin-left:200px}li{margin:0.4em 0}li li{margin:0}hr{display:block;margin:1.75em 0;padding:0;height:1px;border:0;border-top:#efefef 1px solid}blockquote{box-sizing:border-box;margin:1.75em 0 1.75em 0;padding:0 0 0 1.75em;border-left:#4a4a4a 0.4em solid;-moz-box-sizing:border-box}blockquote p{margin:0.8em 0;font-style:italic}blockquote small{display:inline-block;margin:0.8em 0 0.8em 1.5em;color:#ccc;font-size:0.9em}blockquote small:before{content:"\2014 \00A0"}blockquote cite{font-weight:700}blockquote cite a{font-weight:normal}mark{background-color:#fdffb6}code,tt{padding:1px 3px;border:#e3edf3 1px solid;background:#f7fafb;border-radius:2px;white-space:pre-wrap;font-family:Inconsolata, monospace, sans-serif;font-size:0.85em;font-feature-settings:"liga" 0;-webkit-font-feature-settings:"liga" 0;-moz-font-feature-settings:"liga" 0}pre{overflow:auto;box-sizing:border-box;margin:0 0 1.75em 0;padding:10px;width:100%;border:#e3edf3 1px solid;background:#f7fafb;border-radius:3px;white-space:pre;font-family:Inconsolata, monospace, sans-serif;font-size:0.9em;-moz-box-sizing:border-box}pre code,pre tt{padding:0;border:none;background:transparent;white-space:pre-wrap;font-size:inherit}kbd{display:inline-block;margin-bottom:0.4em;padding:1px 8px;border:#ccc 1px solid;background:#f4f4f4;border-radius:4px;box-shadow:0 1px 0 rgba(0, 0, 0, 0.2), 0 1px 0 0 #fff inset;color:#666;text-shadow:#fff 0 1px 0;font-size:0.9em;font-weight:700}table{box-sizing:border-box;margin:1.75em 0;max-width:100%;width:100%;background-color:transparent;-moz-box-sizing:border-box}table th,table td{padding:8px;border-top:#efefef 1px solid;vertical-align:top;text-align:left;line-height:20px}table th{color:#000}table caption + thead tr:first-child th,table caption + thead tr:first-child td,table colgroup + thead tr:first-child th,table colgroup + thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td{border-top:0}table tbody + tbody{border-top:#efefef 2px solid}table table table{background-color:#fff}table tbody > tr:nth-child(odd) > td,table tbody > tr:nth-child(odd) > th{background-color:#f6f6f6}table.plain tbody > tr:nth-child(odd) > td,table.plain tbody > tr:nth-child(odd) > th{background:transparent}iframe,amp-iframe,.fluid-width-video-wrapper{display:block;margin:1.75em 0}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper amp-iframe{margin:0}textarea,select,input{margin:0 0 5px 0;padding:6px 9px;width:260px;outline:0;border:#e7eef2 1px solid;background:#fff;border-radius:4px;box-shadow:none;font-family:"Open Sans", sans-serif;font-size:1.6rem;line-height:1.4em;font-weight:100;-webkit-appearance:none}textarea{min-width:250px;min-height:80px;max-width:340px;width:100%;height:auto}input[type="text"]:focus,input[type="email"]:focus,input[type="search"]:focus,input[type="tel"]:focus,input[type="url"]:focus,input[type="password"]:focus,input[type="number"]:focus,input[type="date"]:focus,input[type="month"]:focus,input[type="week"]:focus,input[type="time"]:focus,input[type="datetime"]:focus,input[type="datetime-local"]:focus,textarea:focus{outline:none;outline-width:0;border:#bbc7cc 1px solid;background:#fff}select{width:270px;height:30px;line-height:30px}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.clearfix{zoom:1}.main-header{position:relative;display:table;overflow:hidden;box-sizing:border-box;width:100%;height:50px;background:#5ba4e5 no-repeat center center;background-size:cover;text-align:left;-webkit-box-sizing:border-box;-moz-box-sizing:border-box}.content{background:#fff;padding-top:15px}.blog-title,.content{margin:auto;max-width:600px}.blog-title a{display:block;padding-right:16px;padding-left:16px;height:50px;color:#fff;text-decoration:none;font-family:"Open Sans", sans-serif;font-size:16px;line-height:50px;font-weight:600}.post{position:relative;margin-top:0;margin-right:16px;margin-left:16px;padding-bottom:0;max-width:100%;border-bottom:#ebf2f6 1px solid;word-wrap:break-word;font-size:0.95em;line-height:1.65em}.post-header{margin-bottom:1rem}.post-title{margin-bottom:0}.post-title a{text-decoration:none}.post-meta{display:block;margin:3px 0 0 0;color:#9eabb3;font-family:"Open Sans", sans-serif;font-size:1.3rem;line-height:2.2rem}.post-meta a{color:#9eabb3;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-meta .author{margin:0;font-size:1.3rem;line-height:1.3em}.post-date{display:inline-block;text-transform:uppercase;white-space:nowrap;font-size:1.2rem;line-height:1.2em}.post-image{margin:0;padding-top:3rem;padding-bottom:30px;border-top:1px #E8E8E8 solid}.post-content amp-img,.post-content amp-anim{position:relative;left:50%;display:block;padding:0;min-width:0;max-width:112%;width:calc(100% + 32px);height:auto;transform:translateX(-50%);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%)}.footnotes{font-size:1.3rem;line-height:1.6em;font-style:italic}.footnotes li{margin:0.6rem 0}.footnotes p{margin:0}.footnotes p a:last-child{text-decoration:none}.site-footer{position:relative;margin:0 auto 20px auto;padding:1rem 15px;max-width:600px;color:rgba(0,0,0,0.5);font-family:"Open Sans", sans-serif;font-size:1.1rem;line-height:1.75em}.site-footer a{color:rgba(0,0,0,0.5);text-decoration:none;font-weight:bold}.site-footer a:hover{border-bottom:#bbc7cc 1px solid}.poweredby{display:block;float:right;width:45%;text-align:right}.copyright{display:block;float:left;width:45%}</style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    

</head>

<body class="amp-template">
    <header class="main-header">
        <nav class="blog-title">
            <a href="../../">Andrew Peng</a>
        </nav>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Transformer [1/3]- Pytorch&#x27;s nn.Transformer</h1>
                <section class="post-meta">
                    <p class="author">by <a href="../../author/andrew/">Andrew Peng</a></p>
                    <time class="post-date" datetime="2019-11-10">2019-11-10</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://andrewpeng.dev/content/images/2019/11/encoder-3-1.png" width="600" height="400" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <p>In part 1 of my series on transformers, I'm going to go over implementing a neural machine translation model using Pytorch's new nn.Transformer module. </p><figure class="kg-card kg-image-card"></figure><p>Transformers, introduced by the paper <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need,</a> inherently don't have a sense of time, They instead rely on <strong>positional encoding </strong>to encode the order of elements. This gives the transformer architecture an important advantage over other language models such as recurrent neural networks: they are parallelizable and easy to expand. This has allowed huge models such as the 1.5 billion parameter <a href="https://openai.com/blog/better-language-models/">GPT-2</a> to achieve state of the art performance on language modelling. </p><h2 id="pytorch">Pytorch</h2><figure class="kg-card kg-image-card"></figure><p>Now, with the release of Pytorch 1.2, we can build transformers in pytorch! We'll go over the basics of the transformer architecture and how to use nn.Transformer. In a transformer, the input sentence goes through an encoder where the sentence gets passed through encoders to become memory. Then the output sentence and memory passes through decoders where it outputs the translated sentence. </p><h3 id="the-encoder">The Encoder</h3><figure class="kg-card kg-image-card"></figure><p>First, we tokenize the input data, pad the array if necessary, and convert the tokens to embeddings. </p><figure class="kg-card kg-code-card"><pre><code class="language-Python">import spacy

# Tokenize sentence
lang_model = spacy.load('en', disable=['tagger', 'parser', 'ner'])
sentence = sentence.lower()
sentence = [tok.text for tok in lang_model.tokenizer(sentence) if tok.text not in punctuation]

# Create a dictionary which maps tokens to indices (train contains all the training sentences)
freq_list = Counter()
    for sentence in train:
        freq_list.update(sentence)

# Convert tokens to indices
indices = [freq_list[word] for word in sentence if word in freq_list]</code></pre><figcaption>Here, I tokenize the sentence using spacy and convert the sentence to indices</figcaption></figure><figure class="kg-card kg-code-card"><pre><code class="language-Python">import torch
from einops import rearrange

self.embed_src = nn.Embedding(vocab_size, d_model)
src = rearrange(indices, 'n s -&gt; s n')
src = self.embed_src(src)</code></pre><figcaption>In the LanguageTransformer class, I create an embedding and embed the batch</figcaption></figure><hr></hr><figure class="kg-card kg-image-card"></figure><p>Now we add the positional encoding to the sentences in order to give some order to the words. In the Attention is All You Need model, they use sine and cosine embeddings to give generalizability to longer sentence sizes. </p><figure class="kg-card kg-code-card"><pre><code class="language-Python">import math 
self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)
src = self.pos_enc(src * math.sqrt(self.d_model))</code></pre><figcaption>In the LanguageTransformer class, I scale src in order to reduce variance then apply the positional encoding</figcaption></figure><figure class="kg-card kg-code-card"><pre><code class="language-Python"># Source: https://pytorch.org/tutorials/beginner/transformer_tutorial
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=100):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_ter
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)</code></pre><figcaption>PositionalEncoding class</figcaption></figure><hr></hr><figure class="kg-card kg-image-card"></figure><p>Masking in the encoder is required to make sure any padding doesn't contribute to the self-attention mechanism. In Pytorch, this is done by passing src_key_padding_mask to the transformer. For the example, this looks like [False, False, False, False, False, False, False, True, True, True] where the True positions should be masked. The output of the encoder is called memory. </p><figure class="kg-card kg-code-card"><pre><code class="language-Python">for i, sentence in enumerate(batch):
        masks.append([False for _ in range(len(sentence))] + [True for _ in range(seq_length - len(sentence))])
        batch[i] = sentence + [0 for _ in range(seq_length - len(sentence))]</code></pre><figcaption>Padding and masking is taken care of in the dataset class</figcaption></figure><hr></hr><h3 id="the-decoder">The Decoder</h3><figure class="kg-card kg-image-card"></figure><p>Now we can move onto the decoder architecture. The initial steps are very similar to that of the encoder. We embed and pass all but the very last token of each sentence into the decoders. </p><figure class="kg-card kg-code-card"><pre><code class="language-Python">self.embed_tgt = nn.Embedding(vocab_size, d_model)

tgt_inp = tgt[:, :-1]
tgt = rearrange(tgt_inp, 'n t -&gt; t n')
tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))</code></pre><figcaption>In the LanguageTransformer class, we embed and encode the target sequence</figcaption></figure><hr></hr><figure class="kg-card kg-image-card"></figure><p>We then pass these sequences through m decoders.  In each decoder, the sequences propagate through self attention and then attention with the memory (from the encoder). So the decoder requires 3 masks: </p><ol><li>tgt_mask: Used in the self-attention, it ensures the decoder doesn't look at future tokens from a given subsequence. This looks like [[0 -inf -inf ... ], [0 0 -inf ...] ... [0 0 0 ...]]</li><li>tgt_key_padding_mask: Also used in the self-attention, it ensures that the padding in the target sequence isn't accounted for. </li><li>memory_key_padding_mask: Used in the attention with the memory, it ensures the padding in the memory isn't used. This is the same as the src_key_padding_mask</li></ol><figure class="kg-card kg-code-card"><pre><code class="language-Python">def gen_nopeek_mask(length):
    mask = rearrange(torch.triu(torch.ones(length, length)) == 1, 'h w -&gt; w h')
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))

    return mask

memory_key_padding_mask = src_key_padding_mask.clone()
tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to('cuda')</code></pre><figcaption>This is in the train method. src_key_padding_mask and tgt_key_padding_mask is expected from the dataloader</figcaption></figure><p>Afterwards, we pass each of the output sequences through a fully connected layer that outputs a probability for each token in the vocab size. </p><figure class="kg-card kg-code-card"><pre><code class="language-Python">self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)

output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,
                                  tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)
output = rearrange(output, 't n e -&gt; n t e')
output = self.fc(output)</code></pre><figcaption>This is in the LanguageTransformer class</figcaption></figure><p>And here is the completed LanguageTransformer class!</p><pre><code class="language-Python">class LanguageTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos_dropout, trans_dropout):
        super().__init__()
        self.d_model = d_model
        self.embed_src = nn.Embedding(vocab_size, d_model)
        self.embed_tgt = nn.Embedding(vocab_size, d_model)
        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)

        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)
        self.fc = nn.Linear(d_model, vocab_size)

    def forward(self, src, tgt, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_mask):
        src = rearrange(src, 'n s -&gt; s n')
        tgt = rearrange(tgt, 'n t -&gt; t n')
        src = self.pos_enc(self.embed_src(src) * math.sqrt(self.d_model))
        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))

        output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,
                                  tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)
        output = rearrange(output, 't n e -&gt; n t e')
        return self.fc(output)</code></pre><hr></hr><h3 id="results">Results</h3><p>I used the tatoeba dataset, a small dataset with around 160000 english to french language pairs available <a href="http://www.manythings.org/anki/">here</a>. </p><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>A relatively small dataset with short sentences</figcaption></figure><p>Here are the results of training for 20 epochs:</p><figure class="kg-card kg-image-card"></figure><p>My model achieves a validation loss of 0.99. However, it starts overfitting around epoch 15 based from the validation loss being higher than the train loss. And finally, some results of translating sentences:</p><p>I am giving you a gift.: Je vous donne un cadeau.<br />
How did you find that?: Comment l'as-tu trouvée?<br />
I'm going to run to your house.: Je vais courir à votre maison.</p>
<hr></hr><h2 id="further-research">Further Research</h2><p>Some improvements that could be made:</p><ol><li>Using beam search to translate sentences</li><li>Running the model on larger datasets </li><li>Using torchtext instead of hacking my own dataset class to get more consistent batches</li><li>Using smoothened loss</li></ol><p>My code is located <a href="https://github.com/andrewpeng02/transformer-translation">here</a>. </p>

            </section>

        </article>
    </main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="../../">Andrew Peng</a> &copy; 2020</section>
        <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>
</body>
</html>
