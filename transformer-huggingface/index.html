<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>Transformer [2/2]- PytorchTransformers Library</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="stylesheet" type="text/css" href="../assets/css/screen.css%3Fv=64f443a096.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/global.css%3Fv=64f443a096.css" />
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,600,700" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Playfair+Display:700" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Source+Serif+Pro:400,700" rel="stylesheet">

    <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
    <link rel="canonical" href="/" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="amp/" />
    
    <meta property="og:site_name" content="Andrew Peng" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Transformer [2/2]- PytorchTransformers Library" />
    <meta property="og:description" content="In part 2 of my post, I&#x27;m going to go over huggingface&#x27;s pytorch transformers library." />
    <meta property="og:url" content="https://andrewpeng.dev/transformer-huggingface/" />
    <meta property="og:image" content="https://andrewpeng.dev/content/images/2019/11/transformers_logo_name-3.png" />
    <meta property="article:published_time" content="2019-11-26T03:12:41.000Z" />
    <meta property="article:modified_time" content="2020-02-16T20:31:28.000Z" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Transformer [2/2]- PytorchTransformers Library" />
    <meta name="twitter:description" content="In part 2 of my post, I&#x27;m going to go over huggingface&#x27;s pytorch transformers library." />
    <meta name="twitter:url" content="https://andrewpeng.dev/transformer-huggingface/" />
    <meta name="twitter:image" content="https://andrewpeng.dev/content/images/2019/11/transformers_logo_name-3.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Andrew Peng" />
    <meta property="og:image:width" content="98" />
    <meta property="og:image:height" content="88" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Andrew Peng",
        "logo": "https://static.ghost.org/v1.0.0/images/ghost-logo.svg"
    },
    "author": {
        "@type": "Person",
        "name": "Andrew Peng",
        "url": "https://andrewpeng.dev/author/andrew/",
        "sameAs": []
    },
    "headline": "Transformer [2/2]- PytorchTransformers Library",
    "url": "https://andrewpeng.dev/transformer-huggingface/",
    "datePublished": "2019-11-26T03:12:41.000Z",
    "dateModified": "2020-02-16T20:31:28.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://andrewpeng.dev/content/images/2019/11/transformers_logo_name-3.png",
        "width": 98,
        "height": 88
    },
    "description": "In part 2 of my post, I&#x27;m going to go over huggingface&#x27;s pytorch transformers library.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://andrewpeng.dev/"
    }
}
    </script>

    <meta name="generator" content="Ghost 2.28" />
    <link rel="alternate" type="application/rss+xml" title="Andrew Peng" href="../rss/" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism.min.css"></link>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

</head>
<body class="post-template">

    <div class="wrapper_minimal">

        <header class="header" role="banner">
    <h1 class="header__title">
        <a title="Andrew Peng" href='../'>Andrew Peng</a>
    </h1>
    <nav class="header__nav" role="navigation">
        <a class="header__link" href="../">Writing</a>
        <a class="header__link" href="../project">Projects</a>
        <a class="header__link" href="../about">About</a>
    </nav>
</header>

<main class="main_minimal" role="main">
    <article class="article__wrapper" role="article">

        <header>
            <h1 class="post__title">Transformer [2/2]- PytorchTransformers Library</h1>
        </header>

        <section class="post__content-wrapper">
            <div class="post__content">
                <p>In part 2 of my post, I'm going to go over huggingface's pytorch transformers library located <a href="https://github.com/huggingface/transformers">here</a>. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="../content/images/2019/11/transformers_logo_name-1.png" class="kg-image"></figure><!--kg-card-end: image--><p>This library provides over 30 pretrained state of the art transformer models on a variety of languages. Alike convolutional neural networks, transformers trained on a different linguistic dataset can be easily retrained on different language datasets. Using pretrained models rather than starting from scratch greatly reduces training time and can sometimes increase accuracy over training a model from scratch. </p><p>In this tutorial, I'll be fine-tuning a DistilBert model to predict the sentiment of IMDB movie reviews. DistilBert is a smaller version of the BERT model, allowing it to get most of the performance of BERT for much less training. More details are located in huggingface's <a href="https://medium.com/huggingface/distilbert-8cf3380435b5">blog post</a>. </p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="implementation">Implementation</h2><h3 id="data-preprocessing">Data Preprocessing</h3><p>I'm using an <a href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews">IMDB movie reviews dataset</a>, which has a list of movie reviews as well as either a "positive" or "negative" sentiment. </p><p>To use huggingface's pretrained models, we have to use their provided tokenizer. Because acquiring the sentiment from a review isn't reliant on stop words such as 'and', 'or', or 'at', we remove them. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">from bs4 import BeautifulSoup
from transformers import DistilBertTokenizer

from nltk.corpus import stopwords

stopwords = stopwords.words('english')
distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

review = BeautifulSoup(review, "html.parser").get_text()
review = review.lower()
review = distilbert_tokenizer.tokenize(review)

# Remove stopwords/punctuation
review = [w for w in review if w not in stopwords and contains_alphabet(w)]</code></pre><figcaption>Preprocess data class</figcaption></figure><!--kg-card-end: code--><p>Now in the dataset class, we attach the start token [CLF], insert padding, and convert the tokenized words to indicies. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">seq_length = 256
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

review = ['[CLF]'] + review[:seq_length - 1]
review = review + ['[PAD]' for _ in range(self.seq_length - len(review))]
review = tokenizer.convert_tokens_to_ids(review)</code></pre><figcaption>Dataset class</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h3 id="training">Training</h3><p>Instantiating the DistilBert model is as simple as importing the class. </p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python">model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2).to('cuda')
lrs = [{'params': model.distilbert.parameters(), 'lr': kwargs['lr_transformer']},
           {'params': model.pre_classifier.parameters()},
           {'params': model.classifier.parameters()}]
optim = Adam(lrs, lr=kwargs['lr_classifier'], eps=kwargs['eps'])</code></pre><figcaption>Train class. It may take a while to download the pretrained model. Here, I apply different learning rates to the transformer and classifier to achieve better results.&nbsp;</figcaption></figure><!--kg-card-end: code--><p>To train the model, all we have to do is pass the reviews and labels to the model and we get our losses back!</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-Python ">reviews = reviews.to('cuda')
labels = labels.to('cuda').long()

optim.zero_grad()
loss = model(reviews, labels=labels)[0]

loss.backward()
optim.step()</code></pre><figcaption>Train class. After I get the losses from every minibatch, I backpropagate.</figcaption></figure><!--kg-card-end: code--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="results">Results</h2><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="../content/images/2019/11/Steps-and-Losses.png" class="kg-image"></figure><!--kg-card-end: image--><p>After 3 epochs, the pretrained transformer reaches a validation loss of 0.262 and a validation accuracy of 0.9021. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="../content/images/2019/11/Model-and-Validation-Accuracy.png" class="kg-image"></figure><!--kg-card-end: image--><p>I trained other common linguistic models including an LSTM and a transformer implemented using nn.Transformer. As shown in the graph, the huggingface transformer still edges out in terms of accuracy.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="../content/images/2019/11/Model-and-Training-time-per-epoch.png" class="kg-image"></figure><!--kg-card-end: image--><p></p><p>DistilBert took the longest time to train by far with approximately — min, likely because it has the most amount of parameters. This is followed by nn.Transformer and then by the LSTM. </p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="further-research">Further Research</h2><p>Some further improvements that could be made to this model:</p><ol><li>Improving the dataloader by allowing for variable length batches in order to reduce the amount of wasted memory spent on padding</li><li>Optimizing the parameters further, such as by adding differential learning rates</li></ol><p>My code is located <a href="https://github.com/andrewpeng02/imdb-sentiment">here</a>. </p>
            </div>
        </section>



    </article>

    <nav class="post__footer" role="navigation">
        <div class="footer__wrapper">
            <div class="footer__wrapper-inner">

                <div class="footer__link-prev">
                    <a class="footer__link-anchor" href="../transformer-pytorch/" rel="prev">
                        <object class="footer__link-icon-left">
                            <embed src="../assets/icons/arrow-left.svg" />
                        </object>
                        <p class="footer__link-label">Previous</p>
                        <h3 class="footer__title-article">Transformer [1/2]- Pytorch&#x27;s nn.Transformer</h3>
                    </a>
                </div>

                <div class="footer__link-next">
                    <a class="footer__link-anchor" href="../gpu-monitor-intellij-plugin/" rel="next">
                        <object class="footer__link-icon-right">
                            <embed src="../assets/icons/arrow-right.svg" />
                        </object>
                        <p class="footer__link-label">Next</p>
                        <h3 class="footer__title-article">GPU Monitor IntelliJ Plugin</h3>
                    </a>
                </div>

            </div>
        </div>
    </nav>
</main>


<footer class="footer" role="contentinfo">
    <div>Copyright &copy; 2020 <a href="../">Andrew Peng</a> &bull; All rights reserved.</div>
</footer>

    </div>

    <script>
        var images = document.querySelectorAll('.kg-gallery-image img');
        images.forEach(function (image) {
            var container = image.closest('.kg-gallery-image');
            var width = image.attributes.width.value;
            var height = image.attributes.height.value;
            var ratio = width / height;
            container.style.flex = ratio + ' 1 0%';
        })
    </script>

    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="https://andrewpeng.dev/assets/built/jquery.fitvids.js?v=64f443a096"></script>


    

    <!-- Core -->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/prism.min.js"></script>

<!-- All individual language files -->
<!-- Python syntax highlighting-->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-python.min.js"></script>

</body>
</html>